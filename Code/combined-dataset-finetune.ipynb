{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetune import Classifier\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1278129, 10)\n",
      "index                                                             0\n",
      "COMPLAINT_ID                                        US_CHICAGO_1725\n",
      "CITY                                                     US_CHICAGO\n",
      "COMPLAINT DATE                                           10/03/2011\n",
      "DEPT_311                                         health_environment\n",
      "CODE_311                           permits issued by doe work order\n",
      "CATEGORY_MAIN                                           environment\n",
      "CATEGORY_SUB                                    environment_general\n",
      "COMPLAINT_1       QUESTIONABLE BUSINESS PRACTICES REGARDING OILS...\n",
      "COMPLAINT_2       [INSPECTION LOG #: 1723 03-OCT-11 18:55:00] TH...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "filePath = \"../Datasets/combined_trainingdata_20181013.tsv\"\n",
    "data = pandas.read_csv(filePath,sep='\\t')\n",
    "print(data.shape)\n",
    "print(data.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no complaint 1 data. Create Complaint column that has (complaint 2 or complaint 1).\n",
    "We are also doing pre-processing. This involves removing punctuation, making everything lowercase, replacing numbers with N, cutting everything after 512 chars, concatenating two types of complaints when they are for Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMPLAINT DATE', 'DEPT_311', 'COMPLAINT_1', 'COMPLAINT_2']\n",
      "(14025, 10)\n",
      "(964291, 10)\n",
      "(13579, 10)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns[data.isna().any()].tolist())\n",
    "print(data[data.COMPLAINT_1.isna()].shape)\n",
    "print(data[data.COMPLAINT_2.isna()].shape)\n",
    "print(data[(data.COMPLAINT_1.isna()) & (data.COMPLAINT_2.isna())].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n",
      "(0, 10)\n",
      "(0, 10)\n"
     ]
    }
   ],
   "source": [
    "dataFiltered = data.dropna(subset = [\"COMPLAINT_1\"])\n",
    "print(dataFiltered[(dataFiltered.COMPLAINT_1.isna())].shape)\n",
    "print(dataFiltered[(dataFiltered.COMPLAINT_1.isna()) & (dataFiltered.COMPLAINT_2.isna())].shape)\n",
    "print(dataFiltered[dataFiltered.COMPLAINT_1 == \"\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "translator = str.maketrans('', '', string.punctuation) # To remove punctuation\n",
    "\n",
    "def preProcess(complaintStart):\n",
    "    complaint = complaintStart[:512] # cut to 512 characters max\n",
    "    complaint = re.sub(\"\\d\",\"N\", complaint) # remove numbers\n",
    "    complaint = complaint.lower().translate(translator) # lower case and remove the punctuation\n",
    "    complaint = complaint.replace(\"\\n\",\" \").strip() # remove starting and trailing white spaces\n",
    "    if re.search('[a-zA-Z]', complaint) is None:# if there are no letters in the complaint, return empty, will be removed in later processing\n",
    "        return \"\"\n",
    "    return complaint\n",
    "\n",
    "def getComplaint(row):\n",
    "    complaint2 = row.get(\"COMPLAINT_2\")\n",
    "    if not pandas.isnull(complaint2):\n",
    "        if \"[INSPECTION LOG #:\" in complaint2: # Remove inspection log section from C2\n",
    "            complaintStrippedList = complaint2.split(\"]\")[1:]\n",
    "            complaintFinal = \"]\".join(complaintStrippedList)\n",
    "        else:\n",
    "            complaintFinal = complaint2\n",
    "        if row.get(\"CITY\")==\"US_CHICAGO\": # if Chicago, concatenate the two\n",
    "            complaintFinal = row.get(\"COMPLAINT_1\") + \" \"+ complaintFinal\n",
    "        complaintProcessed = preProcess(complaintFinal)\n",
    "        if complaintProcessed == \"\" or re.search('[a-zA-Z]', complaintProcessed) is None: # if nothing or no letters\n",
    "            return preProcess(row.get(\"COMPLAINT_1\"))\n",
    "        return complaintProcessed\n",
    "    complaintProcessed = preProcess(row.get(\"COMPLAINT_1\"))\n",
    "    return complaintProcessed\n",
    "\n",
    "results = dataFiltered.apply(lambda row: getComplaint (row),axis=1)\n",
    "print(results[results.isna()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 11)\n"
     ]
    }
   ],
   "source": [
    "dataFiltered[\"complaint\"] = results\n",
    "print(dataFiltered[dataFiltered.complaint.isna()].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strip white spaces from CATEGORY_SUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataFiltered[\"CATEGORY_SUB\"] = dataFiltered[\"CATEGORY_SUB\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter data only to ones with at least 10 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1084456, 11)\n"
     ]
    }
   ],
   "source": [
    "mask = (dataFiltered[\"complaint\"].str.len() >=10)\n",
    "dataFiltered = dataFiltered.loc[mask]\n",
    "print(dataFiltered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the classes to only ones with more than 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['environment_general' 'environment_dumping' 'environment_air_pollution'\n",
      " 'environment_abandoned_site' 'publicorder_noise_complaint'\n",
      " 'environment_asbestos' 'environment_hazardous_material'\n",
      " 'planning_general' 'infrastructure_general' 'environment_water_pollution'\n",
      " 'environment_recycling' 'street_repair' 'housing_general'\n",
      " 'environment_overgrowth' 'housing_health_code' 'street_sewar'\n",
      " 'environment_garbage_collection' 'housing_safety'\n",
      " 'environment_abandoned_vehicle' 'governance_signage' 'street_general'\n",
      " 'environment_litter' 'street_urgent_repair' 'street_sidewalk'\n",
      " 'environment_dead_animal' 'street_cleaning' 'environemnt_dead_animal'\n",
      " 'vandalism_general' 'planning_construction' 'planning_unsafe_environment'\n",
      " 'infrastructure_water' 'infrastructure_water_repair'\n",
      " 'environnment_hazardous_material' 'vandalism_graffiti' 'street_lighting'\n",
      " 'publichealth_general' 'publichealth_school_hygiene' 'fire_general'\n",
      " 'street_parking' 'housing_mold' 'publichealth_pests' 'housing_pests'\n",
      " 'street_slippery' 'publicorder_general' 'governance_general'\n",
      " 'publichealth_animal_feces' 'fire_equipment_broken' 'publichealth_animal'\n",
      " 'governance_parks_and_rec' 'publichealth_public_building_hygiene'\n",
      " 'governance_it' 'fire_code_violation' 'governance_community' 'fire_risks'\n",
      " 'goveranance_general' 'publictransit_bus_service' 'street_drainage'\n",
      " 'publichealth_restaurant_hygiene' 'infrastructure_power'\n",
      " 'street_roadkill' 'publicorder_suspicious_behavior'\n",
      " 'publicorder_drug_activity' 'publicorder_dangerous_driving']\n"
     ]
    }
   ],
   "source": [
    "print(dataFiltered.CATEGORY_SUB.unique())\n",
    "aggregation = {\"complaint\":\"count\"}\n",
    "aggregatedByLabel = dataFiltered.groupby(\"CATEGORY_SUB\").agg(aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      complaint\n",
      "CATEGORY_SUB                                   \n",
      "publicorder_suspicious_behavior               1\n",
      "publicorder_drug_activity                     6\n",
      "publicorder_dangerous_driving                 7\n",
      "environnment_hazardous_material              15\n",
      "publichealth_public_building_hygiene         20\n",
      "publictransit_bus_service                    26\n",
      "fire_code_violation                          27\n",
      "environment_water_pollution                  34\n",
      "publichealth_school_hygiene                  38\n",
      "goveranance_general                          49\n",
      "governance_community                         61\n",
      "governance_it                                81\n",
      "fire_risks                                  103\n",
      "housing_pests                               157\n",
      "environemnt_dead_animal                     268\n",
      "street_roadkill                             415\n",
      "planning_construction                       422\n",
      "vandalism_general                           453\n",
      "publichealth_animal_feces                   771\n",
      "infrastructure_power                        809\n",
      "publichealth_animal                         818\n",
      "fire_equipment_broken                       870\n",
      "publichealth_restaurant_hygiene            1606\n",
      "street_parking                             2219\n",
      "environment_asbestos                       2711\n",
      "infrastructure_water_repair                3155\n",
      "housing_safety                             3729\n",
      "publichealth_pests                         3913\n",
      "environment_abandoned_site                 4152\n",
      "housing_mold                               4282\n",
      "...                                         ...\n",
      "street_drainage                            6400\n",
      "environment_general                        7469\n",
      "governance_parks_and_rec                   7524\n",
      "housing_health_code                        7805\n",
      "infrastructure_water                       8250\n",
      "publicorder_noise_complaint                8645\n",
      "street_general                             8717\n",
      "publicorder_general                        8788\n",
      "street_slippery                            9332\n",
      "planning_unsafe_environment               10082\n",
      "governance_signage                        10234\n",
      "fire_general                              10819\n",
      "environment_dead_animal                   11553\n",
      "environment_litter                        16575\n",
      "publichealth_general                      17171\n",
      "environment_air_pollution                 17755\n",
      "street_sidewalk                           19658\n",
      "infrastructure_general                    24056\n",
      "planning_general                          25650\n",
      "street_urgent_repair                      27076\n",
      "environment_abandoned_vehicle             32280\n",
      "governance_general                        35966\n",
      "street_lighting                           41301\n",
      "environment_dumping                       41305\n",
      "vandalism_graffiti                        44267\n",
      "housing_general                           47189\n",
      "environment_recycling                     61300\n",
      "street_repair                             98151\n",
      "environment_overgrowth                    98471\n",
      "environment_garbage_collection           274040\n",
      "\n",
      "[63 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(aggregatedByLabel.sort_values((\"complaint\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1084456, 11)\n",
      "(1084091, 11)\n"
     ]
    }
   ],
   "source": [
    "goodLabels = aggregatedByLabel[aggregatedByLabel[\"complaint\"]>100]\n",
    "goodLabelsList = goodLabels.index.tolist()\n",
    "print(dataFiltered.shape)\n",
    "dataGoodLabels = dataFiltered[dataFiltered[\"CATEGORY_SUB\"].isin(goodLabelsList)]\n",
    "print(dataGoodLabels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SUB to MAIN mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'environemnt_dead_animal': 'environment',\n",
       " 'environment_abandoned_site': 'environment',\n",
       " 'environment_abandoned_vehicle': 'environment',\n",
       " 'environment_air_pollution': 'environment',\n",
       " 'environment_asbestos': 'environment',\n",
       " 'environment_dead_animal': 'environment',\n",
       " 'environment_dumping': 'environment',\n",
       " 'environment_garbage_collection': 'environment',\n",
       " 'environment_general': 'environment',\n",
       " 'environment_hazardous_material': 'environment',\n",
       " 'environment_litter': 'environment',\n",
       " 'environment_overgrowth': 'environment',\n",
       " 'environment_recycling': 'environment',\n",
       " 'fire_equipment_broken': 'fire',\n",
       " 'fire_general': 'fire',\n",
       " 'fire_risks': 'fire',\n",
       " 'governance_general': 'governance',\n",
       " 'governance_parks_and_rec': 'governance',\n",
       " 'governance_signage': 'governance',\n",
       " 'housing_general': 'housing',\n",
       " 'housing_health_code': 'housing',\n",
       " 'housing_mold': 'housing',\n",
       " 'housing_pests': 'housing',\n",
       " 'housing_safety': 'housing',\n",
       " 'infrastructure_general': 'infrastructure',\n",
       " 'infrastructure_power': 'infrastructure',\n",
       " 'infrastructure_water': 'infrastructure',\n",
       " 'infrastructure_water_repair': 'infrastructure',\n",
       " 'planning_construction': 'planning',\n",
       " 'planning_general': 'planning',\n",
       " 'planning_unsafe_environment': 'planning',\n",
       " 'publichealth_animal': 'public_health',\n",
       " 'publichealth_animal_feces': 'public_health',\n",
       " 'publichealth_general': 'public_health',\n",
       " 'publichealth_pests': 'public_health',\n",
       " 'publichealth_restaurant_hygiene': 'public_health',\n",
       " 'publicorder_general': 'public_order',\n",
       " 'publicorder_noise_complaint': 'public_order',\n",
       " 'street_cleaning': 'street',\n",
       " 'street_drainage': 'street',\n",
       " 'street_general': 'street',\n",
       " 'street_lighting': 'street',\n",
       " 'street_parking': 'street',\n",
       " 'street_repair': 'street',\n",
       " 'street_roadkill': 'street',\n",
       " 'street_sewar': 'street',\n",
       " 'street_sidewalk': 'street',\n",
       " 'street_slippery': 'street',\n",
       " 'street_urgent_repair': 'street',\n",
       " 'vandalism_general': 'vandalism',\n",
       " 'vandalism_graffiti': 'vandalism'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsMap = dataGoodLabels[[\"CATEGORY_MAIN\", \"CATEGORY_SUB\"]].drop_duplicates()\n",
    "labelsMap = labelsMap.set_index(\"CATEGORY_SUB\").to_dict()[\"CATEGORY_MAIN\"]\n",
    "labelsMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1084091, 2)\n"
     ]
    }
   ],
   "source": [
    "trainingData = dataGoodLabels[[\"complaint\", \"CATEGORY_SUB\"]]\n",
    "print(type(trainingData))\n",
    "print(trainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stratified sample to test with (use 10% of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108410,)\n"
     ]
    }
   ],
   "source": [
    "_, sampleX, _, sampleY = train_test_split(trainingData.complaint, trainingData.CATEGORY_SUB, test_size=0.1, random_state=42, stratify=trainingData.CATEGORY_SUB)\n",
    "print(sampleY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking stratification (it seems to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['environment_garbage_collection' 'street_repair' 'housing_general'\n",
      " 'street_urgent_repair' 'street_lighting' 'environment_overgrowth'\n",
      " 'environment_dumping' 'fire_general' 'street_drainage' 'planning_general'\n",
      " 'street_general' 'governance_signage' 'environment_air_pollution'\n",
      " 'environment_recycling' 'housing_health_code' 'vandalism_graffiti'\n",
      " 'environment_abandoned_site' 'publichealth_restaurant_hygiene'\n",
      " 'infrastructure_water_repair' 'infrastructure_general' 'street_slippery'\n",
      " 'street_sewar' 'environment_hazardous_material'\n",
      " 'environment_abandoned_vehicle' 'street_sidewalk' 'infrastructure_water'\n",
      " 'publichealth_general' 'publicorder_general' 'street_parking'\n",
      " 'housing_mold' 'environment_litter' 'environment_asbestos'\n",
      " 'governance_general' 'environment_dead_animal' 'housing_safety'\n",
      " 'infrastructure_power' 'publicorder_noise_complaint'\n",
      " 'environment_general' 'street_cleaning' 'planning_unsafe_environment'\n",
      " 'fire_risks' 'governance_parks_and_rec' 'vandalism_general'\n",
      " 'publichealth_pests' 'publichealth_animal_feces' 'street_roadkill'\n",
      " 'fire_equipment_broken' 'publichealth_animal' 'planning_construction'\n",
      " 'environemnt_dead_animal' 'housing_pests']\n",
      "                                 complaint\n",
      "CATEGORY_SUB                              \n",
      "fire_risks                              10\n",
      "housing_pests                           16\n",
      "environemnt_dead_animal                 27\n",
      "street_roadkill                         41\n",
      "planning_construction                   42\n",
      "vandalism_general                       45\n",
      "publichealth_animal_feces               77\n",
      "infrastructure_power                    81\n",
      "publichealth_animal                     82\n",
      "fire_equipment_broken                   87\n",
      "publichealth_restaurant_hygiene        161\n",
      "street_parking                         222\n",
      "environment_asbestos                   271\n",
      "infrastructure_water_repair            315\n",
      "housing_safety                         373\n",
      "publichealth_pests                     391\n",
      "environment_abandoned_site             415\n",
      "housing_mold                           428\n",
      "street_sewar                           470\n",
      "street_cleaning                        495\n",
      "environment_hazardous_material         575\n",
      "street_drainage                        640\n",
      "environment_general                    747\n",
      "governance_parks_and_rec               752\n",
      "housing_health_code                    781\n",
      "infrastructure_water                   825\n",
      "publicorder_noise_complaint            865\n",
      "street_general                         872\n",
      "publicorder_general                    879\n",
      "street_slippery                        933\n",
      "planning_unsafe_environment           1008\n",
      "governance_signage                    1023\n",
      "fire_general                          1082\n",
      "environment_dead_animal               1155\n",
      "environment_litter                    1658\n",
      "publichealth_general                  1717\n",
      "environment_air_pollution             1776\n",
      "street_sidewalk                       1966\n",
      "infrastructure_general                2406\n",
      "planning_general                      2565\n",
      "street_urgent_repair                  2708\n",
      "environment_abandoned_vehicle         3228\n",
      "governance_general                    3597\n",
      "street_lighting                       4130\n",
      "environment_dumping                   4131\n",
      "vandalism_graffiti                    4427\n",
      "housing_general                       4719\n",
      "environment_recycling                 6130\n",
      "street_repair                         9815\n",
      "environment_overgrowth                9847\n",
      "environment_garbage_collection       27404\n"
     ]
    }
   ],
   "source": [
    "stratSampleDF = pandas.concat([sampleX,sampleY], axis = 1)\n",
    "stratSampleDF.head()\n",
    "print(stratSampleDF.CATEGORY_SUB.unique())\n",
    "aggregationStrat = {\"complaint\":\"count\"}\n",
    "aggregatedByLabelStrat = stratSampleDF.groupby(\"CATEGORY_SUB\").agg(aggregationStrat)\n",
    "print(aggregatedByLabelStrat.sort_values((\"complaint\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split sample into train/test (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86728,)\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(sampleX, sampleY, test_size=0.2, random_state=42, stratify=sampleY)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model in finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:986: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Shapes are always computed; don't use the compute_shapes as it has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                             | 15/41198 [00:12<2:57:57,  3.86it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-5e9fd14e918b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m               \u001b[0;31m# Load base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# Finetune base model on custom data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It took :\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/finetune/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;34m\"\"\" An alias for finetune. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/finetune/classifier.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(self, X, Y, batch_size)\u001b[0m\n\u001b[1;32m     55\u001b[0m                            \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mexamples\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0mto\u001b[0m \u001b[0meach\u001b[0m \u001b[0mGPU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_eval_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/finetune/base.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(self, Xs, Y, batch_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0marr_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/finetune/base.py\u001b[0m in \u001b[0;36m_training_loop\u001b[0;34m(self, arr_encoded, Y, batch_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_dropout\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDROPOUT_ON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/finetune/base.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, feed_dict, *tensors)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         ]\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         return {\n\u001b[1;32m    191\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "model = Classifier(max_length=512, val_interval=3000, verbose = True)               # Load base model\n",
    "model.fit(trainX.tolist(), trainY.tolist())          # Finetune base model on custom data\n",
    "duration = time.time()-start\n",
    "print(\"It took :\"+str(duration)+ \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"combined_model_20181018\")                   # Serialize the model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model and see prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<finetune.classifier.Classifier object at 0x7fbab7c25ef0>\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/function.py:986: calling Graph.create_op (from tensorflow.python.framework.ops) with compute_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Shapes are always computed; don't use the compute_shapes as it has no effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model = Classifier.load(\"../models/combined_model_20181018\")\n",
    "print(model)\n",
    "predictions = model.predict(testX.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18102\n",
      "21682\n",
      "Accuracy on Main: 0.834886080619869\n"
     ]
    }
   ],
   "source": [
    "mainPredictions = []\n",
    "for pred in predictions:\n",
    "    mainPredictions.append(labelsMap[pred])\n",
    "\n",
    "mainTestY = []\n",
    "for testLabel in testY.tolist():\n",
    "    mainTestY.append(labelsMap[testLabel])\n",
    "    \n",
    "correctMain = 0\n",
    "countMain = 0\n",
    "for i, complaint in enumerate(testX.tolist()):\n",
    "    correctMain += int(mainPredictions[i] == mainTestY[i])\n",
    "    countMain +=1\n",
    "print(correctMain)\n",
    "print(countMain)\n",
    "print(\"Accuracy on Main: \"+str(correctMain*1.0/countMain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16240\n",
      "21682\n",
      "Accuracy on Sub: 0.7490083940595886\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "correct = 0\n",
    "count = 0\n",
    "testYList = testY.tolist()\n",
    "for i, complaint in enumerate(testX.tolist()):\n",
    "    correct += int(predictions[i] == testYList[i])\n",
    "    count +=1\n",
    "print(correct)\n",
    "print(count)\n",
    "print(\"Accuracy on Sub: \"+str(correct*1.0/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
